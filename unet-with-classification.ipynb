{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10922333,"sourceType":"datasetVersion","datasetId":6790583},{"sourceId":10933663,"sourceType":"datasetVersion","datasetId":6798845}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch # For model\nimport torch.nn as nn # For model\nfrom torchvision import models\nimport torchvision.transforms.functional as TF # For resizing if not multiple of 16\nfrom torchvision.models import ResNet18_Weights\nimport torch.nn.functional as F\n\nimport os\nos.environ[\"NO_ALBUMENTATIONS_UPDATE\"] = \"1\"\nimport random\nimport albumentations as A # For training\nfrom albumentations.pytorch import ToTensorV2 # For training\nfrom tqdm import tqdm # For training\nimport torch.optim as optim # For training\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.metrics import accuracy_score\nimport cv2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:00:51.358584Z","iopub.execute_input":"2025-03-07T18:00:51.358852Z","iopub.status.idle":"2025-03-07T18:01:00.497644Z","shell.execute_reply.started":"2025-03-07T18:00:51.358818Z","shell.execute_reply":"2025-03-07T18:01:00.496974Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(DoubleConv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        return self.conv(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:01:05.694356Z","iopub.execute_input":"2025-03-07T18:01:05.694761Z","iopub.status.idle":"2025-03-07T18:01:05.702469Z","shell.execute_reply.started":"2025-03-07T18:01:05.694720Z","shell.execute_reply":"2025-03-07T18:01:05.701366Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class UNET(nn.Module):\n    def __init__(self, in_channels=3, out_channels=1, features=[32,64,128,256]):\n        super(UNET, self).__init__()\n\n        # Used as arrays for dynamic layers\n        self.downs = nn.ModuleList()\n        self.ups = nn.ModuleList()\n\n        # Used for fixed downsampling\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        # Down part of UNET\n        for feature in features:\n            self.downs.append(DoubleConv(in_channels, feature))\n            in_channels = feature\n        \n        # Up part of UNET\n        for feature in reversed(features):\n            self.ups.append(\n                nn.ConvTranspose2d(\n                feature*2, feature, kernel_size=2, stride=2)\n            )\n            self.ups.append(DoubleConv(feature*2,feature))\n        \n        # Bottom part of UNET\n        self.bottom = DoubleConv(features[-1],features[-1]*2)\n\n        # Final ouput of UNET\n        self.final_conv = nn.Conv2d(features[0],out_channels, kernel_size=1)\n    \n    def forward(self, x):\n        skip_connections = []\n\n        # Running the Down part of UNET\n        for down in self.downs:\n            x = down(x)\n            # Skip connections saved to be used while upsampling\n            skip_connections.append(x)\n            x = self.pool(x)\n\n        # Reverse the skip_connections list\n        skip_connections = skip_connections[::-1]\n\n        # Running the bottom part of UNET\n        x = self.bottom(x)\n\n        # Running the Up part of UNET\n        for index in range(0, len(self.ups), 2):\n            # Only up used\n            x = self.ups[index](x)\n            skip_connection = skip_connections[index//2]\n\n            # Concatenate connection along feature axis\n            # x will always be smaller or equal\n            # like 51 X 51 -> (max pool) -> 25 X 25 -> (up) -> 50 X 50\n            if x.shape != skip_connection.shape:\n                x = TF.resize(x, size=skip_connection.shape[2:])\n            skip_added = torch.cat((skip_connection,x), dim=1)\n\n            # DoubleConv used then\n            x = self.ups[index+1](skip_added)\n        \n        return torch.sigmoid(self.final_conv(x)) # To keep values between 0 & 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:01:08.772070Z","iopub.execute_input":"2025-03-07T18:01:08.772408Z","iopub.status.idle":"2025-03-07T18:01:08.780500Z","shell.execute_reply.started":"2025-03-07T18:01:08.772383Z","shell.execute_reply":"2025-03-07T18:01:08.779756Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class UNET_with_Classifier(nn.Module):\n    def __init__(self,in_channels=3, out_channels=1, num_classes=10):\n        super(UNET_with_Classifier, self).__init__()\n\n        self.unet = UNET(in_channels=in_channels,out_channels=out_channels)\n\n        # Using resnet for classification of cropped image\n        resnet = models.resnet18(ResNet18_Weights.DEFAULT)\n        resnet.fc = nn.Linear(resnet.fc.in_features,num_classes)\n        self.classifier = resnet\n    \n    def bounding_box_and_cropping(self, x, mask):\n        # Making bounding boxes for each image\n        boxes = []\n        for idx in range (0,len(mask)):\n            m = mask[idx,0]\n            indices = (m > 0.5).nonzero(as_tuple=False)\n            if indices.size(0) > 0:\n                y_min, x_min = indices[:, 0].min(), indices[:, 1].min()\n                y_max, x_max = indices[:, 0].max(), indices[:, 1].max()\n                boxes.append((y_min, x_min, y_max, x_max))\n            else:\n                boxes.append((0, 0, mask.shape[2], mask.shape[3]))\n\n        # Cropping and resizing for classification\n        crops = []\n        for idx in range (0, len(mask)):\n            img = x[idx]\n            y_min, x_min, y_max, x_max = boxes[idx]\n            if (y_max - y_min > 0) and (x_max - x_min > 0):\n                cropped = img[:, y_min:y_max, x_min:x_max]\n                resized = F.interpolate(cropped.unsqueeze(0), size=(224, 224), mode='bilinear', align_corners=False)\n            else :\n                resized = F.interpolate(img.unsqueeze(0), size=(224, 224), mode='bilinear', align_corners=False)\n            crops.append(resized.squeeze(0))   \n        return torch.stack(crops)\n    \n    def forward(self, x):\n        mask = self.unet(x)\n        cropped_image = self.bounding_box_and_cropping(x, mask)\n        flood_level_logits = self.classifier(cropped_image)\n        return mask, flood_level_logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:01:11.511378Z","iopub.execute_input":"2025-03-07T18:01:11.511685Z","iopub.status.idle":"2025-03-07T18:01:11.522285Z","shell.execute_reply.started":"2025-03-07T18:01:11.511661Z","shell.execute_reply":"2025-03-07T18:01:11.521175Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_image(path):\n    image = Image.open(path).convert(\"RGB\")\n    return np.array(image, dtype=np.float32)  # (H, W, 3)\n\ndef load_mask(path):\n    mask = Image.open(path).convert(\"L\")\n    return np.array(mask, dtype=np.float32)[..., np.newaxis]  # (H, W, 1)\n\nclass FloodDataset(Dataset):\n    def __init__(self, image_paths, mask_paths, levels, transforms=None):\n        self.image_paths = image_paths\n        self.mask_paths = mask_paths\n        self.levels = levels\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image = load_image(self.image_paths[idx])\n        mask = load_mask(self.mask_paths[idx])\n        level = torch.tensor(self.levels[idx], dtype=torch.long)\n\n    # Ensure mask is always [H, W]\n        if mask.ndim == 3 and mask.shape[2] == 1:\n            mask = mask.squeeze(-1)\n        elif mask.ndim != 2:\n            raise ValueError(f\"Unexpected mask shape at index {idx}: {mask.shape}\")\n        if image.shape[:2] != mask.shape[:2]:\n            mask = cv2.resize(mask, (image.shape[1], image.shape[0]), interpolation=cv2.INTER_NEAREST)\n\n        if self.transforms:\n            augmented = self.transforms(image=image, mask=mask)\n            image = augmented['image']\n            mask = augmented['mask']\n\n    # After transforms, enforce [1, H, W]\n        if mask.ndim == 2:\n            mask = mask.unsqueeze(0)\n        elif mask.ndim == 3 and mask.shape[0] != 1:\n            raise ValueError(f\"Mask has unexpected shape after transform at index {idx}: {mask.shape}\")\n\n        return {\n            'image': image,\n            'mask': mask,\n            'level': level\n        }\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T19:11:53.504685Z","iopub.execute_input":"2025-03-07T19:11:53.505042Z","iopub.status.idle":"2025-03-07T19:11:53.512849Z","shell.execute_reply.started":"2025-03-07T19:11:53.505012Z","shell.execute_reply":"2025-03-07T19:11:53.512050Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_fn(loader, model, optimizer, segmentation_loss_fn, classification_loss_fn, device):\n    model.train()\n    total_loss = 0\n    total_seg_loss = 0\n    total_cls_loss = 0\n\n    loop = tqdm(loader)\n\n    for batch in loop:\n        images = batch['image'].to(device)\n        true_masks = batch['mask'].to(device)\n        flood_levels = batch['level'].to(device)\n\n        if true_masks.shape[1] != 1:\n            true_masks = true_masks.permute(0, 3, 1, 2)\n        true_masks = true_masks/255.0\n\n        # Forward pass\n        pred_masks, pred_levels = model(images)\n\n        # Compute losses\n        seg_loss = segmentation_loss_fn(pred_masks, true_masks)\n        cls_loss = classification_loss_fn(pred_levels, flood_levels)\n        loss = seg_loss + 5*cls_loss\n\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        total_seg_loss += seg_loss.item()\n        total_cls_loss += cls_loss.item()\n\n        loop.set_postfix(loss=loss.item(), seg_loss=seg_loss.item(), cls_loss=cls_loss.item())\n\n    avg_loss = total_loss / len(loader)\n    avg_seg_loss = total_seg_loss / len(loader)\n    avg_cls_loss = total_cls_loss / len(loader)\n\n    print(f\"Train Loss: {avg_loss:.4f} | Segmentation Loss: {avg_seg_loss:.4f} | Classification Loss: {avg_cls_loss:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:01:16.914304Z","iopub.execute_input":"2025-03-07T18:01:16.914593Z","iopub.status.idle":"2025-03-07T18:01:16.920651Z","shell.execute_reply.started":"2025-03-07T18:01:16.914570Z","shell.execute_reply":"2025-03-07T18:01:16.919707Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def test_fn(loader, model, segmentation_loss_fn, classification_loss_fn, saved_samples, device):\n    model.eval()\n    total_loss = 0\n    total_seg_loss = 0\n    total_cls_loss = 0\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        loop = tqdm(loader)\n\n        for batch in loop:\n            images = batch['image'].to(device)\n            true_masks = batch['mask'].to(device)\n            flood_levels = batch['level'].to(device)\n            \n            if true_masks.ndim == 3:\n                true_masks = true_masks.unsqueeze(1)\n                \n            if true_masks.shape[1] != 1:\n                true_masks = true_masks.permute(0, 3, 1, 2)\n            \n            true_masks_normalized = true_masks/255.0\n\n            pred_masks, pred_levels = model(images)\n\n            seg_loss = segmentation_loss_fn(pred_masks, true_masks_normalized)\n            cls_loss = classification_loss_fn(pred_levels, flood_levels)\n            loss = seg_loss + 5*cls_loss\n\n            total_loss += loss.item()\n            total_seg_loss += seg_loss.item()\n            total_cls_loss += cls_loss.item()\n\n            loop.set_postfix(loss=loss.item(), seg_loss=seg_loss.item(), cls_loss=cls_loss.item())\n\n            # Save samples for visualization\n            if len(saved_samples) < 10:\n                for i in range(images.size(0)):\n                    if len(saved_samples) >= 10:\n                        break\n                    img = images[i].cpu().permute(1, 2, 0).numpy()\n                    true_mask = true_masks[i].cpu().squeeze().numpy()\n                    pred_mask = pred_masks[i].cpu().squeeze().numpy()\n                    pred_mask = (pred_mask > 0.5).astype(float)\n                    saved_samples.append((img, true_mask, pred_mask))\n                    \n            preds = torch.argmax(pred_levels, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(flood_levels.cpu().numpy())\n\n    avg_loss = total_loss / len(loader)\n    avg_seg_loss = total_seg_loss / len(loader)\n    avg_cls_loss = total_cls_loss / len(loader)\n\n    print(f\"\\nTest Loss: {avg_loss:.4f} | Segmentation Loss: {avg_seg_loss:.4f} | Classification Loss: {avg_cls_loss:.4f}\")\n    model.train()\n    return all_preds, all_labels\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:01:21.149815Z","iopub.execute_input":"2025-03-07T18:01:21.150248Z","iopub.status.idle":"2025-03-07T18:01:21.162388Z","shell.execute_reply.started":"2025-03-07T18:01:21.150211Z","shell.execute_reply":"2025-03-07T18:01:21.161432Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = UNET_with_Classifier().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\nsegmentation_loss_fn = nn.BCELoss()\nclassification_loss_fn = nn.CrossEntropyLoss()\nepochs = 25\nnumber_of_images = 2000\n\nimage_dir = \"/kaggle/input/flooded-cars-and-masks/cropped_car_small/cropped_car_small\" # Path to images\nmask_dir = \"/kaggle/input/flooded-cars-and-masks/cropped_mask_small/cropped_mask_small\" # Path to masks\nimage_filenames = os.listdir(image_dir)\n# random.shuffle(image_filenames)\n# selected_images = image_filenames[:number_of_images]\n\nimage_paths = []\nmask_paths = []\nlevels = []\nfor filename in image_filenames:\n    # Image path\n    image_path = os.path.join(image_dir, filename)\n    \n    # Convert image filename to mask filename (change .jpg to .png)\n    mask_filename = filename.replace(\".jpg\", \".png\")\n    mask_path = os.path.join(mask_dir, mask_filename)\n\n    # Extract the level from filename\n    level = int(filename.split('_')[2])  # The third part, i.e. after second '_' (index 2)\n\n    image_paths.append(image_path)\n    mask_paths.append(mask_path)\n    levels.append(level)\n\n# Split 80% train, 20% test\ntrain_img_paths, test_img_paths, train_mask_paths, test_mask_paths, train_levels, test_levels = train_test_split(\n    image_paths,\n    mask_paths,\n    levels,\n    test_size=0.2,        # 20% test\n    random_state=42,      # For reproducibility\n    shuffle=True\n)\n\n# Define transformations\ntrain_transforms = A.Compose([\n    A.Resize(320, 320),\n    A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n    ToTensorV2(),\n])\ntrain_dataset = FloodDataset(train_img_paths, train_mask_paths, train_levels, transforms=train_transforms)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n\nfor epoch in range(epochs):\n    print(f\"Epoch [{epoch+1}/{epochs}]\")\n    train_fn(train_loader, model, optimizer, segmentation_loss_fn, classification_loss_fn, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:01:25.012872Z","iopub.execute_input":"2025-03-07T18:01:25.013205Z","iopub.status.idle":"2025-03-07T18:24:53.036498Z","shell.execute_reply.started":"2025-03-07T18:01:25.013178Z","shell.execute_reply":"2025-03-07T18:24:53.035348Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# import numpy as np\n# import matplotlib.pyplot as plt\n# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n\n# # Paths\n# test_img_dir2 = \"/kaggle/input/flooded-cars-2/images_flood_small\"\n# test_mask_dir2 = \"/kaggle/input/flooded-cars-and-masks/cropped_mask_small/cropped_mask_small\"\n\n# # Load filenames\n# image_filenames = sorted(os.listdir(test_img_dir2))\n# mask_filenames = sorted(os.listdir(test_mask_dir2))\n\n# min_size = min(len(image_filenames), len(mask_filenames))\n\n# image_filenames = image_filenames[:min_size]\n# mask_filenames = mask_filenames[:min_size]\n\n# # Create paths\n# test_img_paths2 = [os.path.join(test_img_dir2, filename) for filename in image_filenames]\n# test_mask_paths2 = [os.path.join(test_mask_dir2, filename) for filename in mask_filenames]\n\n# # Dummy flood levels (adjust if you have actual levels)\n# test_levels2 = np.zeros(min_size)\n\n# Dataset and DataLoader\ntest_dataset = FloodDataset(test_img_paths, test_mask_paths, test_levels2, transforms=train_transforms)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=True, num_workers=4)\nfor batch in test_loader:\n    images = batch['image']\n    true_masks = batch['mask']\n    flood_levels = batch['level']\n\n    print(f\"Image shape: {images.shape}\")\n    print(f\"Mask shape: {true_masks.shape}\")\n    print(f\"Flood levels shape: {flood_levels.shape}\")\n    break\n\n# Run test\nsaved_samples = []\nall_preds, all_labels = test_fn(test_loader, model, segmentation_loss_fn, classification_loss_fn, saved_samples, device)\n\n# Plot sample predictions\nfor idx, (img, true_mask, pred_mask) in enumerate(saved_samples):\n    fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n    axs[0].imshow(img)\n    axs[0].set_title(f'Sample {idx+1}: Input Image')\n    axs[1].imshow(true_mask, cmap='gray')\n    axs[1].set_title('True Mask')\n    axs[2].imshow(pred_mask, cmap='gray')\n    axs[2].set_title('Predicted Mask')\n    plt.show()\n\n# Confusion Matrix\ncm = confusion_matrix(all_labels, all_preds)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisp.plot(cmap=\"Blues\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n\n# Accuracy\naccuracy = accuracy_score(all_labels, all_preds)\nprint(f\"Classification Accuracy: {accuracy * 100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T19:34:53.047394Z","iopub.execute_input":"2025-03-07T19:34:53.047706Z","iopub.status.idle":"2025-03-07T19:35:05.064956Z","shell.execute_reply.started":"2025-03-07T19:34:53.047682Z","shell.execute_reply":"2025-03-07T19:35:05.064135Z"}},"outputs":[],"execution_count":null}]}